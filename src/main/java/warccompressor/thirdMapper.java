/*
This file is part of Hadoop WARC Compressor.

Hadoop WARC Compressor is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Hadoop WARC Compressor is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Hadoop WARC Compressor.  If not, see <http://www.gnu.org/licenses/>.
*/

package warccompressor;

import java.io.IOException;
import java.math.*;
import java.util.*;
import java.lang.StringBuilder;
import java.util.concurrent.ThreadLocalRandom;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.lib.input.LineRecordReader;

public class thirdMapper extends Mapper<LongWritable, Text, Text, Text>
{		
	private ArrayList<ArrayList<String>> cluster = new ArrayList<ArrayList<String>>();
	private ArrayList<String> assigned = new ArrayList<String>();
	private ArrayList<Integer> assignedIndex = new ArrayList<Integer>();
	private int c = 0;
	private int r = 0;
	
	protected void setup(Context context) throws IOException, InterruptedException
	{
		// assign random offset to groups
		// to avoid collisions with others group generated by different maps
		r = ThreadLocalRandom.current().nextInt(0, 10000000);
	}

	public void map(LongWritable positionOffset, Text hashAndOffset, Context context) throws IOException, InterruptedException
	{
		// input format: <offset1_offset2 precision>
		String[] hno = hashAndOffset.toString().split("\t");
		String[] offsets = hno[0].split("_");
		String similarity = hno[1];
		
		if (similarity.equals("0")) {
			context.write(new Text(offsets[0]),new Text("alone"));
			//System.out.println("0 found "+offsets[0]);
			return;
		}
		
		// search if page is already assigned in a cluster
		int a0 = assigned.indexOf(offsets[0]);
		int a1 = assigned.indexOf(offsets[1]);
		
		ArrayList<String> currentCluster;
		
		int currentIndex = -1;
		
		String lastAdded = offsets[1];
		
		if (similarity.equals("1")) { // not similar enough
			if (!(a0 > -1)) { // if not clustered, send to reducer
				context.write(new Text(offsets[0]),new Text("alone"));
			}
				
			if (!(a1 > -1)) { // if not clustered, send to reducer
				context.write(new Text(offsets[1]),new Text("alone"));
			}
			
			return;
		}
		
		if ((a0 == -1) && (a1 == -1)) { // not assigned both of them
		
			// new cluster creation
			currentCluster = new ArrayList<String>();
			currentCluster.add(offsets[0]);
			currentCluster.add(offsets[1]);
			cluster.add(currentCluster);
			
			// both assigned
			assigned.add(offsets[0]);
			assigned.add(offsets[1]);
			
			// save cluster # of the two offsets for easy finding
			assignedIndex.add(c);
			assignedIndex.add(c);
			
			currentIndex = c;
			
			// sending <offset, cluster number>
			context.write(new Text(offsets[0]),new Text(String.valueOf(r+currentIndex)));
			context.write(new Text(offsets[1]),new Text(String.valueOf(r+currentIndex)));
			
			//System.out.println("Assigned to new cluster "+c);
			
			c++;
		} else if ((a0 > -1) && (a1 == -1)) { // first offset in a cluster, finding them
			// getting cluster index
			currentIndex = assignedIndex.get(a0);
			
			//System.out.println("Togheter "+a0+" "+currentIndex+" "+offsets[0]+"+"+offsets[1]);
			
			// adding the new offset to the existing cluster
			currentCluster = cluster.get(currentIndex);
			currentCluster.add(offsets[1]);
			
			//  saving assegnation & index
			assigned.add(offsets[1]);
			assignedIndex.add(currentIndex);
			
			// sending <offset, cluster number>
			context.write(new Text(offsets[1]),new Text(String.valueOf(r+currentIndex)));
		} else if ((a1 > -1) && (a0 == -1)) { // second offset in a cluster, finding them
			// getting cluster index
			currentIndex = assignedIndex.get(a1);
			
			// adding the new offset to the existing cluster
			currentCluster = cluster.get(currentIndex);
			currentCluster.add(offsets[0]);
			
			//  saving assegnation & index
			assigned.add(offsets[0]);
			assignedIndex.add(currentIndex);
			
			// sending <offset, cluster number>
			context.write(new Text(offsets[0]),new Text(String.valueOf(r+currentIndex)));
		}
	}
}